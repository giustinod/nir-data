{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvZfSy3M+AzEqCJo8aGvu5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giustinod/nir-data/blob/main/Preparazione_dataset_dvoptic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importazioni librerie e definizione funzioni di utilità**"
      ],
      "metadata": {
        "id": "99nUHmVgLSh0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwZpkhQ3sgY4",
        "outputId": "f7468a60-3b50-4ed6-9b01-57789cb9ea57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=a9942e009ef85e388c383f464273671305eb703a2448568cd47ce4e54d95015c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spectral\n",
            "  Downloading spectral-0.23-py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spectral) (1.21.6)\n",
            "Installing collected packages: spectral\n",
            "Successfully installed spectral-0.23\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyod\n",
            "  Downloading pyod-1.0.5.tar.gz (137 kB)\n",
            "\u001b[K     |████████████████████████████████| 137 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyod) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pyod) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.21.6)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.7/dist-packages (from pyod) (0.56.2)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.7.3)\n",
            "Requirement already satisfied: scikit_learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from pyod) (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyod) (1.15.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from pyod) (0.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->pyod) (4.12.0)\n",
            "Requirement already satisfied: setuptools<60 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->pyod) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->pyod) (0.39.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.20.0->pyod) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.51->pyod) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.51->pyod) (4.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyod) (3.0.9)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod) (1.3.5)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels->pyod) (0.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21->statsmodels->pyod) (2022.2.1)\n",
            "Building wheels for collected packages: pyod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.0.5-py3-none-any.whl size=170258 sha256=c5d084f9af871fbe96b872d73ab3497e42b7c7ef7d2b5643c0929cf25d209380\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/98/25/bc9bfccf3907de246c61668bc09c39216ec157f8218aea4c16\n",
            "Successfully built pyod\n",
            "Installing collected packages: pyod\n",
            "Successfully installed pyod-1.0.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cleanlab\n",
            "  Downloading cleanlab-2.1.0-py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.53.0 in /usr/local/lib/python3.7/dist-packages (from cleanlab) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from cleanlab) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from cleanlab) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from cleanlab) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cleanlab) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->cleanlab) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->cleanlab) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->cleanlab) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->cleanlab) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->cleanlab) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->cleanlab) (1.1.0)\n",
            "Installing collected packages: cleanlab\n",
            "Successfully installed cleanlab-2.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikeras[tensorflow]\n",
            "  Downloading scikeras-0.9.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikeras[tensorflow]) (1.0.2)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.7/dist-packages (from scikeras[tensorflow]) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=3 in /usr/local/lib/python3.7/dist-packages (from scikeras[tensorflow]) (4.12.0)\n",
            "Requirement already satisfied: tensorflow>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from scikeras[tensorflow]) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras[tensorflow]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras[tensorflow]) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=0.21->scikeras[tensorflow]) (3.0.9)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.48.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.0.7)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.26.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (14.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (57.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->scikeras[tensorflow]) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.7.0->scikeras[tensorflow]) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.7.0->scikeras[tensorflow]) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->scikeras[tensorflow]) (3.2.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.9.0\n",
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "!pip3 install sklearn\n",
        "!pip3 install spectral\n",
        "!pip3 install pyod\n",
        "!pip3 install cleanlab\n",
        "!pip3 install scikeras[tensorflow]\n",
        "\n",
        "## Import external libraries\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import scipy\n",
        "from scipy import newaxis as nA\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "from spectral import *\n",
        "import spectral.io.envi as envi\n",
        "\n",
        "from pyod.models.copod import COPOD\n",
        "\n",
        "import multiprocessing\n",
        "import time\n",
        "from datetime import datetime\n",
        "import concurrent.futures\n",
        "from multiprocessing import Process\n",
        "\n",
        "def snv(input_data):\n",
        "    # Define a new array and populate it with the corrected data  \n",
        "    # return (input_data - np.mean(input_data, axis=0)) / np.std(input_data, axis=0)\n",
        "    output_data = np.zeros_like(input_data)\n",
        "    for i in range(input_data.shape[0]):\n",
        "      # if np.all(np.std(input_data[i,:]) != 0):\n",
        "      # Apply correction\n",
        "      output_data[i,:] = (input_data[i,:] - np.mean(input_data[i,:])) / (np.std(input_data[i,:]))\n",
        "      # else:\n",
        "      # output_data[i,:] = (input_data[i,:] - np.mean(input_data[i,:]))\n",
        "    return output_data\n",
        "\n",
        "def plot_matrix(matrixToPlot, title='', classList=[]):\n",
        "    # colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]\n",
        "    colors = ['b','c','r','g','w','k']\n",
        "    cm = LinearSegmentedColormap.from_list('custom_RGB_cmap', colors, N=len(classList))\n",
        "    norm = plt.Normalize(vmin=0, vmax=len(classList) - 1)\n",
        "    handles = [plt.Rectangle((0, 0), 0, 0, color=cm(norm(i)), label=str(classList[i])) for i in range(len(classList))]\n",
        "    plt.imshow(matrixToPlot, norm=norm, cmap=cm)\n",
        "    plt.legend(handles=handles, title='Class', bbox_to_anchor=(1, 0), loc='lower right', bbox_transform=plt.gcf().transFigure)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(title + '.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def mlr(x, y, order):\n",
        "    \"\"\"Multiple linear regression fit of the columns of matrix x \n",
        "    (dependent variables) to constituent vector y (independent variables)\n",
        "    \n",
        "    order -     order of a smoothing polynomial, which can be included \n",
        "                in the set of independent variables. If order is\n",
        "                not specified, no background will be included.\n",
        "    b -         fit coeffs\n",
        "    f -         fit result (m x 1 column vector)\n",
        "    r -         residual   (m x 1 column vector)\n",
        "    \"\"\"\n",
        "    if order > 0:\n",
        "        s = scipy.ones((len(y),1))\n",
        "        for j in range(order):\n",
        "            s = scipy.concatenate((s,(scipy.arange(0,1+(1.0/(len(y)-1)),1.0/(len(y)-1))**j)[:x.shape[0],nA]), 1)\n",
        "        X = scipy.concatenate((x, s), 1)\n",
        "    else:\n",
        "        X = x\n",
        "    \n",
        "    #calc fit b=fit coefficients\n",
        "    b = scipy.dot(scipy.dot(scipy.linalg.pinv(scipy.dot(scipy.transpose(X),X)),scipy.transpose(X)),y)\n",
        "    f = scipy.dot(X,b)\n",
        "    r = y - f\n",
        "\n",
        "    return b,f,r\n",
        "\n",
        "def emsc(myarray, order = 1, fit = None):\n",
        "    \"\"\"Extended multiplicative scatter correction (Ref H. Martens)\n",
        "    myarray -   spectral data for background correction\n",
        "    order -     order of polynomial\n",
        "    fit -       if None then use average spectrum, otherwise provide a spectrum\n",
        "                as a column vector to which all others fitted\n",
        "    corr -      EMSC corrected data\n",
        "    mx -        fitting spectrum\n",
        "    \"\"\"\n",
        "    \n",
        "    #choose fitting vector\n",
        "    if fit:\n",
        "        mx = fit\n",
        "    else:\n",
        "        mx = scipy.mean(myarray, axis=0)[:,nA]\n",
        "\n",
        "    #do fitting\n",
        "    corr = scipy.zeros(myarray.shape)\n",
        "    for i in range(len(myarray)):\n",
        "        if np.count_nonzero(myarray[i,:]) > 0:\n",
        "            b,f,r = mlr(mx, myarray[i,:][:,nA], order)\n",
        "            corr[i,:] = scipy.reshape((r/b[0,0]) + mx, (corr.shape[1],))\n",
        "\n",
        "    assert not np.any(np.isnan(corr))\n",
        "    return corr\n",
        "\n",
        "def data_augmentation(scan):\n",
        "  ## Standardize on columns\n",
        "  if np.count_nonzero(scan) > 0:\n",
        "      svn_scan = snv(scan)\n",
        "      msc_scan = emsc(scan)\n",
        "      scan1 = np.concatenate((scan, svn_scan, msc_scan,\\\n",
        "                  savgol_filter(scan, w, polyorder = p, deriv=1),\\\n",
        "                  savgol_filter(svn_scan, w, polyorder = p, deriv=1),\\\n",
        "                  savgol_filter(msc_scan, w, polyorder = p, deriv=1)), axis = 1)\n",
        "  else:\n",
        "      scan1 = np.zeros((scan.shape[0], scan.shape[1] * 6), dtype=np.float32)\n",
        "\n",
        "  print('{} - Data augmentation chunk: {} -> {}'.format(datetime.now(), scan.shape, scan1.shape))\n",
        "  return scan1\n",
        "\n",
        "# Check speculari\n",
        "def outliers_speculari(input_data):\n",
        "\n",
        "  ''' Restituisce gli indici degli spettri che hanno caratteristiche di speculari'''\n",
        "  # per lo spettro IR [1050,1650]nm assumiamo:\n",
        "  IR_delta_max = 0.5\n",
        "  IR_lambda_max = 1.2\n",
        "  # per lo spettro VNIR [[400, 600], [800, 900]]nm assumiamo:\n",
        "  VNIR_lambda_max = 1.2\n",
        "\n",
        "  offset = int((1010 - 400) / 10)\n",
        "  offset2 = int((600 - 400) / 10)\n",
        "  offset3 = int((800 - 400) / 10)\n",
        "  offset4 = int((900 - 400) / 10)\n",
        "  speculari_VNIR = np.where((np.max(input_data[:, :offset2], axis = 1) > VNIR_lambda_max) | (np.max(input_data[:, offset3:offset4], axis = 1) > VNIR_lambda_max)) \n",
        "  speculari_IR = np.where((np.max(input_data[:, offset + 1:], axis = 1) > IR_lambda_max) | ((np.max(input_data[:, offset + 1:], axis = 1) - np.min(input_data[:, offset + 1:], axis = 1)) > IR_delta_max))\n",
        "\n",
        "  # print('Speculari IR: ', speculari_IR)\n",
        "  # print('Speculari VNIR: ', speculari_VNIR)\n",
        "  speculari = np.unique(np.concatenate((speculari_IR[0], speculari_VNIR[0]), axis=0))\n",
        "  # print('Speculari: ', speculari)\n",
        "  return speculari\n",
        "\n",
        "def conv(cl):\n",
        "    if cl == 'Carta':\n",
        "        return 0\n",
        "    if cl == 'VetroCarta':\n",
        "        return 1\n",
        "    if cl == 'Ceramica':\n",
        "        return 2\n",
        "    if cl == 'Opalino':\n",
        "        return 3\n",
        "    if cl == 'Plastica':\n",
        "        return 4\n",
        "    if cl == 'Vetro':\n",
        "        return 5\n",
        "    if cl == 'VetroCeramica':\n",
        "        return 6\n",
        "    if cl == 'BackGround':\n",
        "        return 7\n",
        "    return 9   # default case if x is not found\n",
        "\n",
        "selected_classes = (0, 1, 2, 3, 4, 5, 6, 7)\n",
        "n_classes = len(selected_classes)\n",
        "\n",
        "## Settings for the smooth derivatives using a Savitsky-Golay filter\n",
        "w = 7 ## Sav.Gol window size\n",
        "p = 2 ## Sav.Gol polynomial degree\n",
        "\n",
        "n_features = 182\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dalle immagini HDR estraggo i CSV eliminando i pixel a zero e gli intorni (+ o - 3 wl)**"
      ],
      "metadata": {
        "id": "vNe5AWaILA8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Authenticate to google drive and mount drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root = '/content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/'\n",
        "\n",
        "# mpy = COPOD()\n",
        "\n",
        "# cfr. https://stackoverflow.com/questions/9132114/indexing-numpy-array-neighbours-efficiently\n",
        "def neighbors(x):\n",
        "    return np.array([x-3, x-2, x-1, x, x+1, x+2, x+3])\n",
        "\n",
        "for path, subdirs, files in os.walk(root):\n",
        "    # print('Path: ', path)\n",
        "    for name in files:\n",
        "        if name.endswith('hdr'):\n",
        "            print('{} - Opening {}'.format(datetime.now(), os.path.join(path, name)))\n",
        "            MyImg = envi.open(os.path.join(path, name))\n",
        "            np_img = np.array(MyImg.load())\n",
        "            # print('{} - Image shape: {}'.format(datetime.now(), np_img.shape))\n",
        "            # print('{} - Image rows*cols: {}*{}'.format(datetime.now(), MyImg.nrows, MyImg.ncols))\n",
        "            X = np.resize(np_img, (np_img.shape[0] * np_img.shape[1], np_img.shape[2]))\n",
        "            # print('{} - Conv. to Numpy array {}'.format(datetime.now(), X.shape))\n",
        "            spec_outliers = outliers_speculari(X)\n",
        "            print('{} - Outliers Speculari # {}'.format(datetime.now(), np.unique(spec_outliers).shape))\n",
        "            # fitted = mpy.fit(X)\n",
        "            # c_outliers = mpy.predict(X)\n",
        "            # copod_outliers = np.where(c_outliers >= 1)\n",
        "            # print('{} - Outliers COPOD # {}'.format(datetime.now(), copod_outliers[0].shape))\n",
        "            # mask_outliers = spec_outliers # np.unique(np.concatenate((spec_outliers, copod_outliers[0])))\n",
        "            mask_all_zeroes = np.where(~X.any(axis=1))[0]\n",
        "            nb_all_zeroes = np.unique(np.concatenate([neighbors(i) for i in mask_all_zeroes]))\n",
        "            # print('{} - Rows having all zeroes # {}'.format(datetime.now(), mask_all_zeroes))\n",
        "            # print('{} - Rows having all zeroes in the neighborhood # {} - {}'.format(datetime.now(), nb_all_zeroes, nb_all_zeroes.shape))\n",
        "            nb_all_zeroes = np.delete(nb_all_zeroes, [0, 1, 2, nb_all_zeroes.shape[0]-3, nb_all_zeroes.shape[0]-2, nb_all_zeroes.shape[0]-1], axis = 0)\n",
        "            to_del = np.concatenate([spec_outliers, nb_all_zeroes])\n",
        "            X = np.delete(X, to_del, axis = 0)\n",
        "            # print('{} - Rows having all zeroes in the neighborhood # {} - {}'.format(datetime.now(), nb_all_zeroes, nb_all_zeroes.shape))\n",
        "            print('{} - Result size # {}'.format(datetime.now(), X.shape))\n",
        "            cols = np.concatenate([np.arange(400, 1010, 10), np.arange(1050, 1655, 5)])\n",
        "            df = pd.DataFrame(X, columns = cols)\n",
        "            date_ = name.split('_')[0]\n",
        "            class_ = name.split('_')[1]\n",
        "            class_col = np.full(X.shape[0], class_)\n",
        "            csv = df.insert(0, \"Class\", class_col)\n",
        "            filename = name.replace('hdr', 'csv')\n",
        "            print('{} - csv # {}'.format(datetime.now(), filename))\n",
        "            df.to_csv('/content/drive/MyDrive/Freelancer/GraiNit/dvoptic/training/' + filename, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdgtY5To-w6h",
        "outputId": "c9a095de-0e99-4a8b-89af-632bd9413d11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "2022-09-26 12:32:54.688147 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/23062022_Ceramica_2Merge.hdr\n",
            "2022-09-26 12:32:58.178543 - Outliers Speculari # (3978,)\n",
            "2022-09-26 12:32:58.711602 - Result size # (43566, 182)\n",
            "2022-09-26 12:32:58.723650 - csv # 23062022_Ceramica_2Merge.csv\n",
            "2022-09-26 12:33:06.592579 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Ceramica_Sassi0Merge.hdr\n",
            "2022-09-26 12:33:09.622239 - Outliers Speculari # (890,)\n",
            "2022-09-26 12:33:10.164570 - Result size # (39660, 182)\n",
            "2022-09-26 12:33:10.169433 - csv # 04072022_Ceramica_Sassi0Merge.csv\n",
            "2022-09-26 12:33:17.348045 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Ceramica_Scure0Merge.hdr\n",
            "2022-09-26 12:33:19.891825 - Outliers Speculari # (982,)\n",
            "2022-09-26 12:33:20.412094 - Result size # (44742, 182)\n",
            "2022-09-26 12:33:20.417106 - csv # 04072022_Ceramica_Scure0Merge.csv\n",
            "2022-09-26 12:33:28.444861 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Ceramica_Latterizio1Merge.hdr\n",
            "2022-09-26 12:33:31.067043 - Outliers Speculari # (762,)\n",
            "2022-09-26 12:33:31.673447 - Result size # (20129, 182)\n",
            "2022-09-26 12:33:31.676816 - csv # 04072022_Ceramica_Latterizio1Merge.csv\n",
            "2022-09-26 12:33:35.737327 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Ceramica_Colore1Merge.hdr\n",
            "2022-09-26 12:33:38.726854 - Outliers Speculari # (7496,)\n",
            "2022-09-26 12:33:39.176115 - Result size # (59864, 182)\n",
            "2022-09-26 12:33:39.181780 - csv # 04072022_Ceramica_Colore1Merge.csv\n",
            "2022-09-26 12:33:49.991550 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Ceramica_Bianca2Merge.hdr\n",
            "2022-09-26 12:33:52.641664 - Outliers Speculari # (5834,)\n",
            "2022-09-26 12:33:53.119298 - Result size # (58151, 182)\n",
            "2022-09-26 12:33:53.125036 - csv # 04072022_Ceramica_Bianca2Merge.csv\n",
            "2022-09-26 12:34:03.787001 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Vetro_5Merge.hdr\n",
            "2022-09-26 12:34:06.938121 - Outliers Speculari # (1700,)\n",
            "2022-09-26 12:34:07.441848 - Result size # (53712, 182)\n",
            "2022-09-26 12:34:07.446695 - csv # 04072022_Vetro_5Merge.csv\n",
            "2022-09-26 12:34:17.678953 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Vetro_1Merge.hdr\n",
            "2022-09-26 12:34:20.275442 - Outliers Speculari # (1684,)\n",
            "2022-09-26 12:34:20.752167 - Result size # (60453, 182)\n",
            "2022-09-26 12:34:20.757588 - csv # 04072022_Vetro_1Merge.csv\n",
            "2022-09-26 12:34:31.642982 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Plastica_Buona2Merge.hdr\n",
            "2022-09-26 12:34:33.978972 - Outliers Speculari # (29,)\n",
            "2022-09-26 12:34:34.639423 - Result size # (6003, 182)\n",
            "2022-09-26 12:34:34.642227 - csv # 04072022_Plastica_Buona2Merge.csv\n",
            "2022-09-26 12:34:36.191088 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Plastica_Buona1Merge.hdr\n",
            "2022-09-26 12:34:38.743627 - Outliers Speculari # (380,)\n",
            "2022-09-26 12:34:39.338489 - Result size # (29941, 182)\n",
            "2022-09-26 12:34:39.342577 - csv # 04072022_Plastica_Buona1Merge.csv\n",
            "2022-09-26 12:34:45.540381 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Opalino_1Merge.hdr\n",
            "2022-09-26 12:34:49.955999 - Outliers Speculari # (102,)\n",
            "2022-09-26 12:34:50.612602 - Result size # (10272, 182)\n",
            "2022-09-26 12:34:50.615730 - csv # 04072022_Opalino_1Merge.csv\n",
            "2022-09-26 12:34:52.853137 - Opening /content/drive/MyDrive/Freelancer/GraiNit/dvoptic/merge/04072022_Opalino_0Merge.hdr\n",
            "2022-09-26 12:34:55.944622 - Outliers Speculari # (1327,)\n",
            "2022-09-26 12:34:56.487209 - Result size # (33623, 182)\n",
            "2022-09-26 12:34:56.492507 - csv # 04072022_Opalino_0Merge.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predisposizione dataset per training, validazione e test**"
      ],
      "metadata": {
        "id": "Rhly9jHTK0YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import external libraries\n",
        "\n",
        "roots = ['/content/drive/MyDrive/Freelancer/GraiNit/dvoptic/training/',]\n",
        "\n",
        "columns = np.array(['Class'])\n",
        "\n",
        "df_train = pd.DataFrame(data=None, columns=columns)\n",
        "df_test = pd.DataFrame(data=None, columns=columns)\n",
        "df_val = pd.DataFrame(data=None, columns=columns)\n",
        "\n",
        "perc_train = 0.55\n",
        "perc_test = 0.2\n",
        "perc_val = 0.25\n",
        "\n",
        "# dataset ricevuti dal 1 giugno con aggiunta BG 25 maggio\n",
        "perc_class = [0.5, 0.25, 0.25, 1, 1, 0.5, 0.5, 0.13]\n",
        "\n",
        "for r in roots:\n",
        "    for path, subdirs, files in os.walk(r):\n",
        "        for name in files:\n",
        "            if name.endswith('csv'):\n",
        "                df = pd.read_csv(path + '/' + name, header = 0)\n",
        "                m = re.search('04072022_(.+?)_(.+?).csv', name)\n",
        "                class_ = conv(m.group(1))\n",
        "                df.replace(m.group(1), class_, inplace = True)\n",
        "                df.dropna(how='any', inplace = True)\n",
        "                df.rename(columns={'Unnamed: 0':'Class'}, inplace = True)\n",
        "                cols = np.concatenate([['Class'], np.concatenate([np.arange(400, 1010, 10), np.arange(1050, 1655, 5)])])\n",
        "                df.columns = cols\n",
        "                n_rows = df.shape[0]\n",
        "                n_rows_train = round(n_rows * perc_train * perc_class[class_])\n",
        "                n_rows_test = round(n_rows * perc_test * perc_class[class_])\n",
        "                n_rows_val = round(n_rows * perc_val * perc_class[class_])\n",
        "                print('{} ({}): {}, {}'.format(m.group(1), name, str(n_rows), str(n_rows_train)))\n",
        "                df_train = pd.concat([df_train, df.loc[0:n_rows_train,:]], ignore_index = True)\n",
        "                df_test = pd.concat([df_test, df.loc[n_rows_train+1:n_rows_train+n_rows_test,:]], ignore_index = True)\n",
        "                df_val = pd.concat([df_val, df.loc[n_rows_train+n_rows_test+1:n_rows_train+n_rows_test+n_rows_val,:]], ignore_index = True)\n",
        "\n",
        "# removing duplicates\n",
        "df_train.drop_duplicates(inplace = True)\n",
        "df_test.drop_duplicates(inplace = True)\n",
        "df_val.drop_duplicates(inplace = True)\n",
        "\n",
        "print(df_train.groupby(['Class']).count().iloc[:, :1])\n",
        "df_train.to_csv('/content/drive/MyDrive/Freelancer/GraiNit/dvoptic/' + 'DataSetTraining11.csv', index = False)\n",
        "print(df_test.groupby(['Class']).count().iloc[:, :1])\n",
        "df_test.to_csv('/content/drive/MyDrive/Freelancer/GraiNit/dvoptic/' + 'DataSetTest11.csv', index = False)\n",
        "print(df_val.groupby(['Class']).count().iloc[:, :1])\n",
        "df_val.to_csv('/content/drive/MyDrive/Freelancer/GraiNit/dvoptic/' + 'DataSetValidation11.csv', index = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whSa4hTOKx7h",
        "outputId": "c38c4a42-4f5d-4e3a-f7bd-4b0c965f8a29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BackGround (04072022_BackGround_All.csv): 520266, 37199\n",
            "Carta (04072022_Carta_Organico.csv): 42612, 11718\n",
            "VetroCeramica (04072022_VetroCeramica_All.csv): 125934, 34632\n",
            "VetroCarta (04072022_VetroCarta_All.csv): 261280, 35926\n",
            "Carta (04072022_Carta_Cartone.csv): 71459, 19651\n",
            "Ceramica (04072022_Ceramica_Sassi0Merge.csv): 39660, 5453\n",
            "Ceramica (04072022_Ceramica_Scure0Merge.csv): 44742, 6152\n",
            "Ceramica (04072022_Ceramica_Latterizio1Merge.csv): 20129, 2768\n",
            "Ceramica (04072022_Ceramica_Colore1Merge.csv): 59864, 8231\n",
            "Ceramica (04072022_Ceramica_Bianca2Merge.csv): 58151, 7996\n",
            "Vetro (04072022_Vetro_5Merge.csv): 53712, 14771\n",
            "Vetro (04072022_Vetro_1Merge.csv): 60453, 16625\n",
            "Plastica (04072022_Plastica_Buona2Merge.csv): 6003, 3302\n",
            "Plastica (04072022_Plastica_Buona1Merge.csv): 29941, 16468\n",
            "Opalino (04072022_Opalino_1Merge.csv): 10272, 5650\n",
            "Opalino (04072022_Opalino_0Merge.csv): 33623, 18493\n",
            "Ceramica (04072022_Ceramica_2Merge.csv): 43566, 5990\n",
            "         400\n",
            "Class       \n",
            "0      31371\n",
            "1      35555\n",
            "2      36596\n",
            "3      24145\n",
            "4      19772\n",
            "5      31398\n",
            "6      34553\n",
            "7      37200\n",
            "         400\n",
            "Class       \n",
            "0      11370\n",
            "1      13020\n",
            "2      13305\n",
            "3       8779\n",
            "4       7189\n",
            "5      11416\n",
            "6      12294\n",
            "7      13527\n",
            "         400\n",
            "Class       \n",
            "0      14252\n",
            "1      15934\n",
            "2      16632\n",
            "3      10971\n",
            "4       8983\n",
            "5      14271\n",
            "6      15742\n",
            "7      16909\n"
          ]
        }
      ]
    }
  ]
}